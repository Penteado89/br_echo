import os
import pandas as pd
from dotenv import load_dotenv

# âœ… LangChain - versÃµes atualizadas
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document

# === Carregar variÃ¡veis de ambiente ===
print("ğŸ”§ Carregando variÃ¡veis de ambiente do .env...")
load_dotenv()

# === Caminhos ===
CAMINHO_TEXTOS = "dados/corpus_br_echo_v4_cleaned.csv"
CAMINHO_INDEX = "dados/faiss_index_br_echo.index"

# === Carregar corpus base (para fallback, revisÃ£o manual ou debug) ===
print(f"ğŸ“‚ Lendo corpus: {CAMINHO_TEXTOS}")
df = pd.read_csv(CAMINHO_TEXTOS)
print(f"âœ… Total de entradas carregadas: {len(df)}")

if "text" not in df.columns:
    raise ValueError("âŒ A coluna 'text' nÃ£o foi encontrada no CSV.")

textos = df["text"].dropna().astype(str).tolist()
print(f"ğŸ“ Textos vÃ¡lidos para indexaÃ§Ã£o: {len(textos)}")

# === Carregar Embeddings ===
print("ğŸ“ Carregando embeddings com 'BERTimbau' (neuralmind/bert-base-portuguese-cased)...")
embeddings = HuggingFaceEmbeddings(model_name="neuralmind/bert-base-portuguese-cased")

# === Carregar Ã­ndice FAISS ===
if os.path.exists(CAMINHO_INDEX):
    print(f"ğŸ“¦ Carregando Ã­ndice FAISS existente: {CAMINHO_INDEX}")
    vectorstore = FAISS.load_local(CAMINHO_INDEX, embeddings, allow_dangerous_deserialization=True)
else:
    raise FileNotFoundError("ğŸš§ Arquivo de Ã­ndice FAISS nÃ£o encontrado. Abortando execuÃ§Ã£o.")

# === Carregar LLM ===
print("ğŸ¤– Carregando modelo da OpenAI (gpt-3.5-turbo)...")
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# === Pipeline RAG ===
print("ğŸ”— Construindo pipeline RAG com LangChain...")
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    chain_type="stuff"
)

# === FunÃ§Ã£o exportÃ¡vel ===
def consultar_rag(pergunta: str) -> str:
    resposta = rag_chain.run(pergunta)
    return resposta

# === ExecuÃ§Ã£o interativa local ===
if __name__ == "__main__":
    print("\nğŸš€ Pronto! VocÃª pode comeÃ§ar a fazer perguntas ao sistema com base no corpus BR-ECHO.")
    while True:
        pergunta = input("\nâ“ Digite sua pergunta (ou 'sair' para encerrar): ")
        if pergunta.strip().lower() == "sair":
            print("ğŸ›‘ Encerrando o sistema.")
            break

        print(f"ğŸ” Processando pergunta: {pergunta}")
        resposta = consultar_rag(pergunta)
        print("\nğŸ“˜ Resposta baseada no corpus + LLM:")
        print("-" * 50)
        print(resposta)
        print("-" * 50)
