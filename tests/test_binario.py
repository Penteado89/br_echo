import sys
import os
import traceback
from transformers import BertTokenizer, BertForSequenceClassification
import torch

print("ğŸš€ Iniciando test_binario.py")
print("ğŸ“¦ Python version:", sys.version)

# Verifica o diretÃ³rio de trabalho atual
print("ğŸ“ DiretÃ³rio atual:", os.getcwd())

# Caminho para o modelo binÃ¡rio
MODEL_DIR = "modelos/bertimbau_classificador"

try:
    print(f"ğŸ” Verificando arquivos no diretÃ³rio '{MODEL_DIR}':")
    print(os.listdir(MODEL_DIR))
except Exception as e:
    print("âŒ Erro ao listar diretÃ³rio:", e)
    sys.exit(1)

# Carregar tokenizer e modelo
try:
    print("ğŸ”„ Carregando tokenizer...")
    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
    print("âœ… Tokenizer carregado.")

    print("ğŸ”„ Carregando modelo BERT binÃ¡rio...")
    model = BertForSequenceClassification.from_pretrained(MODEL_DIR)
    print("âœ… Modelo carregado.")
except Exception as e:
    print("âŒ Erro ao carregar modelo/tokenizer:")
    traceback.print_exc()
    sys.exit(1)

# Colocar o modelo em modo de avaliaÃ§Ã£o
model.eval()

# Exemplo de texto para teste
texto = "Vamos atacar os infiÃ©is. Allahu Akbar!"
print(f"ğŸ“ Texto de entrada: {texto}")

# TokenizaÃ§Ã£o
try:
    print("ğŸ§ª Tokenizando o texto...")
    inputs = tokenizer(texto, return_tensors="pt", truncation=True, padding=True)
    print("âœ… TokenizaÃ§Ã£o realizada.")
except Exception as e:
    print("âŒ Erro na tokenizaÃ§Ã£o:")
    traceback.print_exc()
    sys.exit(1)

# PrediÃ§Ã£o
try:
    print("âš™ï¸ Realizando prediÃ§Ã£o...")
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()

    print("ğŸ¯ Resultado da prediÃ§Ã£o:", predicted_class)
except Exception as e:
    print("âŒ Erro na prediÃ§Ã£o:")
    traceback.print_exc()
    sys.exit(1)

print("âœ… Teste finalizado com sucesso.")
